## Derin Öğrenme İçin Aktivasyon Fonksiyonlarının Karşılaştırılması :chart_with_upwards_trend: :chart_with_downwards_trend:

Aktivasyon fonksiyonları sinir ağlarında kilit bir rol oynar, bu nedenle daha iyi performans elde etmek için avantajlarını ve dezavantajlarını anlamak esastır.

En iyi bilinen sigmoid fonksiyonuna alternatif olan lineer olmayan aktivasyon fonksiyonlarını tanıtarak işe başlamak gerek. Aktivasyon fonksiyonlarının son performanslarını değerlendirirken birçok farklı koşulun önemi olduğunu unutmamak gerekir. O halde hazırsanız kolları sıvayalım ve biraz ellerimizi kirletelim!

[Derin Öğrenme İçin Aktivasyon Fonksiyonlarının Karşılaştırılması- Google Colab Notebook](https://colab.research.google.com/github/ayyucekizrak/Udemy_DerinOgrenmeyeGiris/blob/master/Aktivasyon_Fonksiyonlarinin_Karsilastirilmasi/Aktivasyon_Fonksiyonlar%C4%B1_Kar%C5%9F%C4%B1la%C5%9Ft%C4%B1rmas%C4%B1.ipynb)

| AKTİVASYON FONKSİYONU   | TEST DOĞRULUĞU | TEST YİTİMİ   |
| -------------           | -------------  | ------------- |
| Sigmoid                 | 0,9866         | 0,0567        |
| Hiperbolik Tanjant      | 0,9905         | 0,0474        |
| ReLU                    | 0,9929         | 0,0401        |
| Leaky ReLU              | 0,9925         | 0,0403        |
| Swish                   | 0,9922         | 0,0412        |

## YAKINDA!!!
### ⭐️Devamı için [blog yazımı okuyun](https://) ya da  [YouTube anlatımını izleyin](https://www.youtube.com/watch?v=ZMkLC-ebIqE&lc=z23ycfpylrbgd35gq04t1aokges1i24xfrpexcolv3yhrk0h00410)! 


<img align="left" src="AktivasyonFonksiyonları.png">
